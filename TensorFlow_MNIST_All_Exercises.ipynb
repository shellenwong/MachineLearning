{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "There are several main adjustments you may try.\n",
    "\n",
    "Please pay attention to the time it takes for each epoch to conclude.\n",
    "\n",
    "Using the code from the lecture as the basis, fiddle with the hyperparameters of the algorithm.\n",
    "\n",
    "1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
    "\n",
    "2. The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.\n",
    "\n",
    "3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "\n",
    "4. Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the string 'sigmoid'.\n",
    "\n",
    "5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'.\n",
    "\n",
    "6. Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?\n",
    "\n",
    "7. Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?\n",
    "\n",
    "8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "\n",
    "9. Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
    "\n",
    "10. Combine all the methods above and try to reach a validation accuracy of 98.5+ percent.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFLow includes a data provider for MNIST that we'll use.\n",
    "# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using\n",
    "# pip install tensorflow-datasets \n",
    "# or\n",
    "# conda install tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember the comment from above\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer \n",
    "\n",
    "# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) \n",
    "# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument\n",
    "# there are other arguments we can specify, which we can find useful\n",
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# we will use this information a bit below and we will store it in mnist_info\n",
    "\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated \n",
    "\n",
    "# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# by default, TF has training and testing datasets, but no validation sets\n",
    "# thus we must split it on our own\n",
    "\n",
    "# we start by defining the number of validation samples as a % of the train samples\n",
    "# this is also where we make use of mnist_info (we don't have to count the observations)\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# let's cast this number to an integer, as a float may cause an error along the way\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "# once more, we'd prefer an integer (rather than the default float)\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "# normally, we would like to scale our data in some way to make the result more numerically stable\n",
    "# in this case we will simply prefer to have inputs between 0 and 1\n",
    "# let's define a function called: scale, that will take an MNIST image and its label\n",
    "def scale(image, label):\n",
    "    # we make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# the method .map() allows us to apply a custom transformation to a given dataset\n",
    "# we have already decided that we will get the validation data from mnist_train, so \n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# finally, we scale and batch the test data\n",
    "# we scale it so it has the same magnitude as the train and validation\n",
    "# there is no need to shuffle it, because we won't be training on the test data\n",
    "# there would be a single batch, equal to the size of the test data\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "# let's also shuffle the data\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n",
    "# then we can't shuffle the whole dataset in one go because we can't fit it all in memory\n",
    "# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them\n",
    "# if BUFFER_SIZE=1 => no shuffling will actually happen\n",
    "# if BUFFER_SIZE >= num samples => shuffling is uniform\n",
    "# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling\n",
    "\n",
    "# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n",
    "# our validation data would be equal to 10% of the training set, which we've already calculated\n",
    "# we use the .take() method to take that many samples\n",
    "# finally, we create a batch with a batch size equal to the total number of validation samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# determine the batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# batch the test data\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "# takes next batch (it is the only batch)\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model\n",
    "When thinking about a deep learning algorithm, we mostly imagine building the model. So, let's do it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the optimizer we'd like to use, \n",
    "# the loss function, \n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "That's where we train the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - loss: 0.3206 - accuracy: 0.9105 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "540/540 - 5s - loss: 0.1344 - accuracy: 0.9604 - val_loss: 0.1131 - val_accuracy: 0.9680\n",
      "Epoch 3/5\n",
      "540/540 - 5s - loss: 0.0969 - accuracy: 0.9710 - val_loss: 0.0972 - val_accuracy: 0.9745\n",
      "Epoch 4/5\n",
      "540/540 - 5s - loss: 0.0752 - accuracy: 0.9776 - val_loss: 0.0841 - val_accuracy: 0.9745\n",
      "Epoch 5/5\n",
      "540/540 - 5s - loss: 0.0609 - accuracy: 0.9812 - val_loss: 0.0681 - val_accuracy: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x229f22494a8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the maximum number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# we fit the model, specifying the\n",
    "# training data\n",
    "# the total number of epochs\n",
    "# and the validation data we just created ourselves in the format: (inputs,targets)\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "As we discussed in the lectures, after training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 1s 1s/step - loss: 0.0897 - accuracy: 0.97 - 1s 1s/step - loss: 0.0897 - accuracy: 0.9728"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09. Test accuracy: 97.28%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly around 97%.\n",
    "\n",
    "Each time the code is rerun, we get a different accuracy as the batches are shuffled, the weights are initialized in a different way, etc.\n",
    "\n",
    "Finally, we have intentionally reached a suboptimal solution, so you can have space to build on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "The width (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset,mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)\n",
    "\n",
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples,tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image=tf.cast(image,tf.float32)\n",
    "    image/=225.\n",
    "    return image,label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "\n",
    "test_data=mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs,validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 14s - loss: 0.2177 - accuracy: 0.9344 - val_loss: 0.1262 - val_accuracy: 0.9618\n",
      "Epoch 2/5\n",
      "540/540 - 12s - loss: 0.0827 - accuracy: 0.9745 - val_loss: 0.0707 - val_accuracy: 0.9768\n",
      "Epoch 3/5\n",
      "540/540 - 14s - loss: 0.0519 - accuracy: 0.9835 - val_loss: 0.0430 - val_accuracy: 0.9873\n",
      "Epoch 4/5\n",
      "540/540 - 12s - loss: 0.0413 - accuracy: 0.9871 - val_loss: 0.0388 - val_accuracy: 0.9878\n",
      "Epoch 5/5\n",
      "540/540 - 12s - loss: 0.0281 - accuracy: 0.9912 - val_loss: 0.0431 - val_accuracy: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63cbb6790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=500\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9768\n",
      "Test loss: 0.08. Test accuracy: 0.98%.\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy can be a little bit increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 \n",
    "The depth of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset,mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)\n",
    "\n",
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "def scale (image,label):\n",
    "    image=tf.cast(image,tf.float32)\n",
    "    image/=225.\n",
    "    return image,label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs,validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 9s - loss: 0.3998 - accuracy: 0.8829 - val_loss: 0.1757 - val_accuracy: 0.9463\n",
      "Epoch 2/5\n",
      "540/540 - 6s - loss: 0.1639 - accuracy: 0.9508 - val_loss: 0.1395 - val_accuracy: 0.9565\n",
      "Epoch 3/5\n",
      "540/540 - 6s - loss: 0.1263 - accuracy: 0.9620 - val_loss: 0.1170 - val_accuracy: 0.9620\n",
      "Epoch 4/5\n",
      "540/540 - 5s - loss: 0.1034 - accuracy: 0.9673 - val_loss: 0.0966 - val_accuracy: 0.9722\n",
      "Epoch 5/5\n",
      "540/540 - 6s - loss: 0.0896 - accuracy: 0.9728 - val_loss: 0.0820 - val_accuracy: 0.9723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63a191310>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs,validation_targets),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9700\n",
      "Test loss: 0.10. Test accuracy:97.00%.\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy:{1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems not obvious improvement of the accuracy.\n",
    "\n",
    "So we should adjust the depth as well as width at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n",
    "\n",
    "The width and depth of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info=tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image=tf.cast(image,tf.float32)\n",
    "    image/=225.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 65s - loss: 0.2367 - accuracy: 0.9287 - val_loss: 0.1480 - val_accuracy: 0.9593\n",
      "Epoch 2/5\n",
      "540/540 - 64s - loss: 0.1093 - accuracy: 0.9688 - val_loss: 0.1089 - val_accuracy: 0.9693\n",
      "Epoch 3/5\n",
      "540/540 - 65s - loss: 0.0814 - accuracy: 0.9771 - val_loss: 0.0708 - val_accuracy: 0.9783\n",
      "Epoch 4/5\n",
      "540/540 - 64s - loss: 0.0627 - accuracy: 0.9822 - val_loss: 0.0909 - val_accuracy: 0.9718\n",
      "Epoch 5/5\n",
      "540/540 - 64s - loss: 0.0558 - accuracy: 0.9840 - val_loss: 0.0756 - val_accuracy: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63a604110>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=1000\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch,validation_data=(validation_inputs,validation_targets),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9772\n",
      "Test loss: 0.09. Test accuracy: 97.72%.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change not so much among the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4\n",
    "\n",
    "Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the string 'sigmoid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset,mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)\n",
    "\n",
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image/=225.\n",
    "    return image,label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 9s - loss: 1.0410 - accuracy: 0.7581 - val_loss: 0.4217 - val_accuracy: 0.8957\n",
      "Epoch 2/5\n",
      "540/540 - 6s - loss: 0.3301 - accuracy: 0.9128 - val_loss: 0.2676 - val_accuracy: 0.9277\n",
      "Epoch 3/5\n",
      "540/540 - 6s - loss: 0.2405 - accuracy: 0.9312 - val_loss: 0.2105 - val_accuracy: 0.9393\n",
      "Epoch 4/5\n",
      "540/540 - 6s - loss: 0.1959 - accuracy: 0.9436 - val_loss: 0.1772 - val_accuracy: 0.9495\n",
      "Epoch 5/5\n",
      "540/540 - 6s - loss: 0.1645 - accuracy: 0.9524 - val_loss: 0.1532 - val_accuracy: 0.9558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63d2fe410>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1610 - accuracy: 0.9524\n",
      "Test loss: 0.16. Test accuracy: 95.24%.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the activation from 'relu' to 'sigmoid'\n",
    "\n",
    "Generally, we should reach an inferior solution. That is because 'relu' is 'clean' the noise in the data. If a value is negative, relu filters it out, while if it is positive, it takes it into account. \n",
    "\n",
    "For the MNIST dataset, we care only about the intensely black and white parts in the images of the digits, so such filtering proves beneficial.\n",
    "\n",
    "The sigmoid does not filter the signals as well as relu, but still reaches a respectable result, like around 95%.\n",
    "\n",
    "Try using 'softmax' activation for all layers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - loss: 2.1939 - accuracy: 0.4535 - val_loss: 1.9750 - val_accuracy: 0.7478\n",
      "Epoch 2/5\n",
      "540/540 - 6s - loss: 1.6098 - accuracy: 0.7548 - val_loss: 1.2562 - val_accuracy: 0.7603\n",
      "Epoch 3/5\n",
      "540/540 - 6s - loss: 1.0076 - accuracy: 0.7649 - val_loss: 0.8413 - val_accuracy: 0.7788\n",
      "Epoch 4/5\n",
      "540/540 - 5s - loss: 0.7432 - accuracy: 0.7875 - val_loss: 0.6879 - val_accuracy: 0.8063\n",
      "Epoch 5/5\n",
      "540/540 - 5s - loss: 0.6387 - accuracy: 0.8153 - val_loss: 0.6223 - val_accuracy: 0.8138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x639ec2610>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='softmax'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='softmax'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6204 - accuracy: 0.8095\n",
      "Test loss: 0.62. Test accuracy: 80.95%.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If change the activation of all hidden layers from 'relu' to 'softmax', the accuracy will decrease very much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5\n",
    "\n",
    "Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info=tfds.load(name='mnist',with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image/=225.\n",
    "    return image,label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs,validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 9s - loss: 0.4063 - accuracy: 0.8875 - val_loss: 0.1947 - val_accuracy: 0.9423\n",
      "Epoch 2/5\n",
      "540/540 - 7s - loss: 0.1629 - accuracy: 0.9515 - val_loss: 0.1395 - val_accuracy: 0.9577\n",
      "Epoch 3/5\n",
      "540/540 - 6s - loss: 0.1211 - accuracy: 0.9636 - val_loss: 0.1135 - val_accuracy: 0.9670\n",
      "Epoch 4/5\n",
      "540/540 - 6s - loss: 0.0952 - accuracy: 0.9709 - val_loss: 0.0910 - val_accuracy: 0.9723\n",
      "Epoch 5/5\n",
      "540/540 - 6s - loss: 0.0774 - accuracy: 0.9768 - val_loss: 0.0885 - val_accuracy: 0.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63c97b090>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9667\n",
      "Test loss: 0.11. Test accuracy: 96.67%.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogically, we can change the activation functions. This time though, we'll use different activators for the different layers.\n",
    "\n",
    "The result is not significantly different. \n",
    "\n",
    "However, if with different width and depth, that may change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6\n",
    "\n",
    "Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)\n",
    "\n",
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "def scale(image,label):\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image/=225.\n",
    "    return image,label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "batch_size=10000\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 - 6s - loss: 2.2107 - accuracy: 0.2194 - val_loss: 1.9982 - val_accuracy: 0.3933\n",
      "Epoch 2/5\n",
      "6/6 - 3s - loss: 1.8799 - accuracy: 0.4570 - val_loss: 1.6574 - val_accuracy: 0.5593\n",
      "Epoch 3/5\n",
      "6/6 - 4s - loss: 1.5286 - accuracy: 0.6159 - val_loss: 1.3028 - val_accuracy: 0.7102\n",
      "Epoch 4/5\n",
      "6/6 - 4s - loss: 1.1825 - accuracy: 0.7441 - val_loss: 0.9843 - val_accuracy: 0.7802\n",
      "Epoch 5/5\n",
      "6/6 - 4s - loss: 0.8843 - accuracy: 0.7976 - val_loss: 0.7457 - val_accuracy: 0.8112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x639550350>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs,validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7213 - accuracy: 0.8233\n",
      "Test loss: 0.72. Test accuracy: 82.33%.\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss,test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the batch_size from 100 to 10000.\n",
    "\n",
    "A bigger batch size results in slower training. That's what we expected from the theory. We are taking advantage of batching becuz of the amazing speed increase.\n",
    "\n",
    "Notice that the validation accuracy starts from a low number and with 5 epochs actually finishes at a lower number. That's becuz there are fewer updates in a single epoch.\n",
    "\n",
    "If try a batch size of 30000 or 50000. That's very close to single batch GD for this problem. \n",
    "\n",
    "Change the max epochs to 100 (for instance), as 5 epochs won't be enough to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 - 3s - loss: 2.3696 - accuracy: 0.0917 - val_loss: 2.2584 - val_accuracy: 0.1598\n",
      "Epoch 2/5\n",
      "2/2 - 2s - loss: 2.2353 - accuracy: 0.1898 - val_loss: 2.1624 - val_accuracy: 0.3148\n",
      "Epoch 3/5\n",
      "2/2 - 2s - loss: 2.1427 - accuracy: 0.3388 - val_loss: 2.0803 - val_accuracy: 0.4288\n",
      "Epoch 4/5\n",
      "2/2 - 2s - loss: 2.0599 - accuracy: 0.4426 - val_loss: 1.9964 - val_accuracy: 0.4917\n",
      "Epoch 5/5\n",
      "2/2 - 2s - loss: 1.9733 - accuracy: 0.5020 - val_loss: 1.9063 - val_accuracy: 0.5282\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8902 - accuracy: 0.5398\n",
      "Test loss: 1.89. Test accuracy: 53.98%\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 30000\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')   \n",
    "                            ])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If change the batch_size to 30000, it is like doing the single batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 - 1s - loss: 2.3073 - accuracy: 0.1555 - val_loss: 2.2128 - val_accuracy: 0.2575\n",
      "Epoch 2/5\n",
      "2/2 - 0s - loss: 2.2080 - accuracy: 0.2656 - val_loss: 2.1215 - val_accuracy: 0.3787\n",
      "Epoch 3/5\n",
      "2/2 - 0s - loss: 2.1145 - accuracy: 0.3873 - val_loss: 2.0183 - val_accuracy: 0.4617\n",
      "Epoch 4/5\n",
      "2/2 - 0s - loss: 2.0101 - accuracy: 0.4761 - val_loss: 1.9024 - val_accuracy: 0.5312\n",
      "Epoch 5/5\n",
      "2/2 - 0s - loss: 1.8939 - accuracy: 0.5425 - val_loss: 1.7793 - val_accuracy: 0.5852\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7660 - accuracy: 0.5995\n",
      "Test loss: 1.77. Test accuracy: 59.95%\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 50000\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')   \n",
    "                            ])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If change the batch size to 50000, then it's like doing single batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7\n",
    "\n",
    "Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test=mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image/=225.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_input, validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54000/54000 - 100s - loss: 0.2501 - accuracy: 0.9263 - val_loss: 12.1413 - val_accuracy: 0.0893\n",
      "Epoch 2/5\n",
      "54000/54000 - 93s - loss: 0.1555 - accuracy: 0.9573 - val_loss: 18.3165 - val_accuracy: 0.0885\n",
      "Epoch 3/5\n",
      "54000/54000 - 94s - loss: 0.1407 - accuracy: 0.9637 - val_loss: 22.2234 - val_accuracy: 0.0895\n",
      "Epoch 4/5\n",
      "54000/54000 - 99s - loss: 0.1316 - accuracy: 0.9658 - val_loss: 25.6107 - val_accuracy: 0.0895\n",
      "Epoch 5/5\n",
      "54000/54000 - 95s - loss: 0.1359 - accuracy: 0.9682 - val_loss: 26.1097 - val_accuracy: 0.0907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6393ffcd0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9590\n",
      "Test loss: 0.17. Test accuracy: 95.90%.\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change batch_size from 100 to 1.\n",
    "\n",
    "A batch size of 1 results in the SGD. It takes the algorithm very little time to process a single batch, as it is one data point, but there're thousands of batches (54000 to be precise), thus the algorithm is actually slow. Remember that this depends on the number of cores that you train on. If you are using a CPU with 4 or 8 cores, you can only train 4 or 8 batches at once. The middle ground (mini_batching such as 100 samples per batch) is optimal.\n",
    "\n",
    "Notice that the validation accuracy starts from high number. That's becuz there're lots of updates in a single epoch. Once the training is over, the accuracy is lower than all other batch sizes (SGD was an approximation).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8\n",
    "\n",
    "Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset,mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)\n",
    "\n",
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image/=225.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "540/540 - 9s - loss: 1.2111 - accuracy: 0.6664 - val_loss: 0.5791 - val_accuracy: 0.8470\n",
      "Epoch 2/50\n",
      "540/540 - 6s - loss: 0.4404 - accuracy: 0.8839 - val_loss: 0.3847 - val_accuracy: 0.8947\n",
      "Epoch 3/50\n",
      "540/540 - 6s - loss: 0.3350 - accuracy: 0.9071 - val_loss: 0.3212 - val_accuracy: 0.9092\n",
      "Epoch 4/50\n",
      "540/540 - 6s - loss: 0.2903 - accuracy: 0.9190 - val_loss: 0.2877 - val_accuracy: 0.9157\n",
      "Epoch 5/50\n",
      "540/540 - 6s - loss: 0.2597 - accuracy: 0.9276 - val_loss: 0.2623 - val_accuracy: 0.9245\n",
      "Epoch 6/50\n",
      "540/540 - 6s - loss: 0.2385 - accuracy: 0.9335 - val_loss: 0.2419 - val_accuracy: 0.9320\n",
      "Epoch 7/50\n",
      "540/540 - 6s - loss: 0.2231 - accuracy: 0.9373 - val_loss: 0.2296 - val_accuracy: 0.9333\n",
      "Epoch 8/50\n",
      "540/540 - 6s - loss: 0.2087 - accuracy: 0.9410 - val_loss: 0.2158 - val_accuracy: 0.9390\n",
      "Epoch 9/50\n",
      "540/540 - 6s - loss: 0.1969 - accuracy: 0.9451 - val_loss: 0.2034 - val_accuracy: 0.9420\n",
      "Epoch 10/50\n",
      "540/540 - 6s - loss: 0.1869 - accuracy: 0.9470 - val_loss: 0.1950 - val_accuracy: 0.9437\n",
      "Epoch 11/50\n",
      "540/540 - 6s - loss: 0.1794 - accuracy: 0.9485 - val_loss: 0.1853 - val_accuracy: 0.9470\n",
      "Epoch 12/50\n",
      "540/540 - 6s - loss: 0.1713 - accuracy: 0.9512 - val_loss: 0.1791 - val_accuracy: 0.9477\n",
      "Epoch 13/50\n",
      "540/540 - 6s - loss: 0.1635 - accuracy: 0.9534 - val_loss: 0.1723 - val_accuracy: 0.9507\n",
      "Epoch 14/50\n",
      "540/540 - 6s - loss: 0.1579 - accuracy: 0.9549 - val_loss: 0.1657 - val_accuracy: 0.9517\n",
      "Epoch 15/50\n",
      "540/540 - 6s - loss: 0.1511 - accuracy: 0.9574 - val_loss: 0.1605 - val_accuracy: 0.9540\n",
      "Epoch 16/50\n",
      "540/540 - 6s - loss: 0.1471 - accuracy: 0.9581 - val_loss: 0.1561 - val_accuracy: 0.9528\n",
      "Epoch 17/50\n",
      "540/540 - 6s - loss: 0.1413 - accuracy: 0.9607 - val_loss: 0.1503 - val_accuracy: 0.9560\n",
      "Epoch 18/50\n",
      "540/540 - 6s - loss: 0.1378 - accuracy: 0.9609 - val_loss: 0.1470 - val_accuracy: 0.9567\n",
      "Epoch 19/50\n",
      "540/540 - 6s - loss: 0.1323 - accuracy: 0.9628 - val_loss: 0.1425 - val_accuracy: 0.9585\n",
      "Epoch 20/50\n",
      "540/540 - 6s - loss: 0.1285 - accuracy: 0.9629 - val_loss: 0.1392 - val_accuracy: 0.9582\n",
      "Epoch 21/50\n",
      "540/540 - 6s - loss: 0.1252 - accuracy: 0.9646 - val_loss: 0.1366 - val_accuracy: 0.9582\n",
      "Epoch 22/50\n",
      "540/540 - 6s - loss: 0.1204 - accuracy: 0.9660 - val_loss: 0.1329 - val_accuracy: 0.9602\n",
      "Epoch 23/50\n",
      "540/540 - 6s - loss: 0.1168 - accuracy: 0.9672 - val_loss: 0.1308 - val_accuracy: 0.9605\n",
      "Epoch 24/50\n",
      "540/540 - 6s - loss: 0.1143 - accuracy: 0.9674 - val_loss: 0.1269 - val_accuracy: 0.9615\n",
      "Epoch 25/50\n",
      "540/540 - 6s - loss: 0.1099 - accuracy: 0.9691 - val_loss: 0.1248 - val_accuracy: 0.9623\n",
      "Epoch 26/50\n",
      "540/540 - 6s - loss: 0.1079 - accuracy: 0.9695 - val_loss: 0.1203 - val_accuracy: 0.9643\n",
      "Epoch 27/50\n",
      "540/540 - 6s - loss: 0.1044 - accuracy: 0.9706 - val_loss: 0.1221 - val_accuracy: 0.9627\n",
      "Epoch 28/50\n",
      "540/540 - 6s - loss: 0.1027 - accuracy: 0.9715 - val_loss: 0.1174 - val_accuracy: 0.9653\n",
      "Epoch 29/50\n",
      "540/540 - 6s - loss: 0.1002 - accuracy: 0.9721 - val_loss: 0.1130 - val_accuracy: 0.9673\n",
      "Epoch 30/50\n",
      "540/540 - 8s - loss: 0.0970 - accuracy: 0.9732 - val_loss: 0.1120 - val_accuracy: 0.9663\n",
      "Epoch 31/50\n",
      "540/540 - 9s - loss: 0.0944 - accuracy: 0.9738 - val_loss: 0.1108 - val_accuracy: 0.9683\n",
      "Epoch 32/50\n",
      "540/540 - 8s - loss: 0.0937 - accuracy: 0.9741 - val_loss: 0.1081 - val_accuracy: 0.9690\n",
      "Epoch 33/50\n",
      "540/540 - 6s - loss: 0.0901 - accuracy: 0.9754 - val_loss: 0.1061 - val_accuracy: 0.9697\n",
      "Epoch 34/50\n",
      "540/540 - 7s - loss: 0.0891 - accuracy: 0.9753 - val_loss: 0.1062 - val_accuracy: 0.9707\n",
      "Epoch 35/50\n",
      "540/540 - 6s - loss: 0.0867 - accuracy: 0.9766 - val_loss: 0.1036 - val_accuracy: 0.9712\n",
      "Epoch 36/50\n",
      "540/540 - 6s - loss: 0.0849 - accuracy: 0.9765 - val_loss: 0.1030 - val_accuracy: 0.9717\n",
      "Epoch 37/50\n",
      "540/540 - 6s - loss: 0.0835 - accuracy: 0.9770 - val_loss: 0.0996 - val_accuracy: 0.9722\n",
      "Epoch 38/50\n",
      "540/540 - 6s - loss: 0.0810 - accuracy: 0.9772 - val_loss: 0.0984 - val_accuracy: 0.9723\n",
      "Epoch 39/50\n",
      "540/540 - 6s - loss: 0.0789 - accuracy: 0.9785 - val_loss: 0.0963 - val_accuracy: 0.9740\n",
      "Epoch 40/50\n",
      "540/540 - 6s - loss: 0.0777 - accuracy: 0.9789 - val_loss: 0.0954 - val_accuracy: 0.9737\n",
      "Epoch 41/50\n",
      "540/540 - 6s - loss: 0.0764 - accuracy: 0.9788 - val_loss: 0.0946 - val_accuracy: 0.9750\n",
      "Epoch 42/50\n",
      "540/540 - 7s - loss: 0.0756 - accuracy: 0.9791 - val_loss: 0.0927 - val_accuracy: 0.9742\n",
      "Epoch 43/50\n",
      "540/540 - 7s - loss: 0.0734 - accuracy: 0.9793 - val_loss: 0.0912 - val_accuracy: 0.9747\n",
      "Epoch 44/50\n",
      "540/540 - 6s - loss: 0.0722 - accuracy: 0.9799 - val_loss: 0.0881 - val_accuracy: 0.9753\n",
      "Epoch 45/50\n",
      "540/540 - 6s - loss: 0.0705 - accuracy: 0.9805 - val_loss: 0.0887 - val_accuracy: 0.9770\n",
      "Epoch 46/50\n",
      "540/540 - 6s - loss: 0.0684 - accuracy: 0.9813 - val_loss: 0.0874 - val_accuracy: 0.9752\n",
      "Epoch 47/50\n",
      "540/540 - 6s - loss: 0.0674 - accuracy: 0.9815 - val_loss: 0.0864 - val_accuracy: 0.9760\n",
      "Epoch 48/50\n",
      "540/540 - 6s - loss: 0.0670 - accuracy: 0.9819 - val_loss: 0.0853 - val_accuracy: 0.9758\n",
      "Epoch 49/50\n",
      "540/540 - 6s - loss: 0.0648 - accuracy: 0.9819 - val_loss: 0.0840 - val_accuracy: 0.9765\n",
      "Epoch 50/50\n",
      "540/540 - 6s - loss: 0.0637 - accuracy: 0.9826 - val_loss: 0.0818 - val_accuracy: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x638e83090>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "custom_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "num_epoch=50\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9698\n",
      "Test loss: 0.10. Test accuracy: 96.98%.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the learning_rate is lower than normal, need to adjust the max_epoch to bigger (try 50).\n",
    "\n",
    "The result is basically the same, but we reach it much slower.\n",
    "\n",
    "While Adam adapts to the problem, if the orders of magnitude are too different, it may not have enough time to adjust accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9\n",
    "Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info=tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image/=225.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs,validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 9s - loss: 0.3159 - accuracy: 0.9055 - val_loss: 0.2445 - val_accuracy: 0.9360\n",
      "Epoch 2/5\n",
      "540/540 - 6s - loss: 0.2111 - accuracy: 0.9412 - val_loss: 0.2186 - val_accuracy: 0.9417\n",
      "Epoch 3/5\n",
      "540/540 - 6s - loss: 0.1904 - accuracy: 0.9474 - val_loss: 0.1982 - val_accuracy: 0.9468\n",
      "Epoch 4/5\n",
      "540/540 - 6s - loss: 0.1773 - accuracy: 0.9515 - val_loss: 0.2002 - val_accuracy: 0.9475\n",
      "Epoch 5/5\n",
      "540/540 - 6s - loss: 0.1695 - accuracy: 0.9539 - val_loss: 0.1708 - val_accuracy: 0.9542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x636438450>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "custom_optimizer=tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9520\n",
      "Test loss: 0.19. Test accuracy: 95.20%.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Adam adapts to the problem, if the orders of magnitude are too different, it may not have time to adjust accordingly. We start overfitting before we can reach a neat solution.\n",
    "\n",
    "Therefore, for this problem, even 0.02 is a HIGH starting learning rate. \n",
    "\n",
    "Try 0.001, 0.0001, 0.00001. If it makes no difference, pick whatever, otherwise it makes sense to fiddle with the learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - loss: 2.1598 - accuracy: 0.2674 - val_loss: 2.0341 - val_accuracy: 0.3998\n",
      "Epoch 2/5\n",
      "540/540 - 6s - loss: 1.8762 - accuracy: 0.5063 - val_loss: 1.7056 - val_accuracy: 0.5982\n",
      "Epoch 3/5\n",
      "540/540 - 6s - loss: 1.5360 - accuracy: 0.6496 - val_loss: 1.3815 - val_accuracy: 0.6878\n",
      "Epoch 4/5\n",
      "540/540 - 6s - loss: 1.2525 - accuracy: 0.7185 - val_loss: 1.1419 - val_accuracy: 0.7395\n",
      "Epoch 5/5\n",
      "540/540 - 6s - loss: 1.0461 - accuracy: 0.7621 - val_loss: 0.9683 - val_accuracy: 0.7755\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9433 - accuracy: 0.7903\n",
      "Test loss: 0.94. Test accuracy: 79.03%.\n"
     ]
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "custom_optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "num_epoch=5\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
    "\n",
    "\n",
    "test_loss, test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy record: \n",
    "\n",
    "    learning_rate = 1: 9%\n",
    "    \n",
    "    learning_rate = 0.001: 95.4%\n",
    "    \n",
    "    learning_rate = 0.0001: 92.84%\n",
    "    \n",
    "    learning_rate = 0.00001: 79.02%\n",
    "    \n",
    "So learning_rate around 0.02 or just let 'Adam' handles it will be great. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10\n",
    "Combine all the methods above and try to reach a validation accuracy of 98.5+ percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)\n",
    "\n",
    "mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image=tf.cast(image, tf.float32)\n",
    "    image/=225.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data=mnist_train.map(scale)\n",
    "test_data=mnist_test.map(scale)\n",
    "\n",
    "\n",
    "buffer_size=10000\n",
    "\n",
    "shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "batch_size=150\n",
    "\n",
    "train_data=train_data.batch(batch_size)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets=next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "360/360 - 1486s - loss: 1.0597 - accuracy: 0.6385 - val_loss: 0.2777 - val_accuracy: 0.9347\n",
      "Epoch 2/10\n",
      "360/360 - 1309s - loss: 0.2227 - accuracy: 0.9470 - val_loss: 0.7366 - val_accuracy: 0.8513\n",
      "Epoch 3/10\n",
      "360/360 - 1341s - loss: 0.3108 - accuracy: 0.9270 - val_loss: 0.2272 - val_accuracy: 0.9575\n",
      "Epoch 4/10\n",
      "360/360 - 1372s - loss: 0.1690 - accuracy: 0.9638 - val_loss: 0.1780 - val_accuracy: 0.9645\n",
      "Epoch 5/10\n",
      "360/360 - 1379s - loss: 0.1861 - accuracy: 0.9644 - val_loss: 0.1268 - val_accuracy: 0.9717\n",
      "Epoch 6/10\n",
      "360/360 - 1371s - loss: 0.1841 - accuracy: 0.9643 - val_loss: 0.1430 - val_accuracy: 0.9720\n",
      "Epoch 7/10\n",
      "360/360 - 1305s - loss: 0.1222 - accuracy: 0.9735 - val_loss: 0.1998 - val_accuracy: 0.9638\n",
      "Epoch 8/10\n",
      "360/360 - 1344s - loss: 0.1166 - accuracy: 0.9749 - val_loss: 0.1315 - val_accuracy: 0.9773\n",
      "Epoch 9/10\n",
      "360/360 - 1373s - loss: 0.0994 - accuracy: 0.9793 - val_loss: 0.0773 - val_accuracy: 0.9778\n",
      "Epoch 10/10\n",
      "360/360 - 1352s - loss: 0.0778 - accuracy: 0.9839 - val_loss: 0.0985 - val_accuracy: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21c03342308>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=28*28\n",
    "output_size=10\n",
    "hidden_layer_size=5000\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "num_epoch=10\n",
    "model.fit(train_data, epochs=num_epoch, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1510 - accuracy: 0.9760\n",
      "Test loss: 0.15. Test accuracy: 97.60%.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy=model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the methods and try to achieve 98.5%+ accuracy. Achieving 98.5% accuracy with the methodology we've seen so far is extremely hard. A more realistic exercise would be to achieve 98%+ accuracy. However, being pushed to the limit (trying to achieve 98.5%), you have probably learned a whole lot about the machine learning process.\n",
    "\n",
    "Here is a link where you can check the results that some leading academics got on the MNIST (using different methodologies):\n",
    "https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results\n",
    "\n",
    "\n",
    "hidden_layer_size=5000 \n",
    "\n",
    "batch_size=150\n",
    "\n",
    "num_epochs=10\n",
    "\n",
    "activation are 'relu'\n",
    "\n",
    "There're better solutions using this methodology, this one is just superior to the one in the lessons. Due to the width and depth of the algorithm, it took like 3 hours and 47 mins to train it.\n",
    "\n",
    "The final accuracy is like 97.60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
